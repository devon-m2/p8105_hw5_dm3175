---
title: "p8105_hw5_dm3175"
author: "Devon Morgan"
date: "11/8/2018"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(purrr)
knitr::opts_chunk$set(
  fig.width = 10,
  fig.asp = .6,
  out.width = "90%"
)
```

# Homework 5

## Problem 1
This problem investigates observation data from a longitudinal study with a control and experimental arm. 

### Load and Clean Data

First, the data from the longitudinal study was loaded and cleaned. The files were originally stored in separate csv files, but were loaded and combined into one dateframe using the `map` function. The `map` functionality iterates over the cells in the file name column to carry out the `read_csv` function, and enters the output into a new column in the data frame. Data was unnested after it was read in. 

Next, data was tidied using the `gather` function to organize all observations by week of observation. Using the information stored in the file name, columns for the `subject_id` and `week` were created. The `group_arm` variable was converted into a factor variable, and the `subject_id` and `week` into numeric. `obs` contains the corresponding observation (numeric). 

```{r message = FALSE}
# Load list of file names into a data frame
longit_file_names = tibble(files = list.files("./data"),
                           path = str_c("./data/", files)) %>% 
  janitor::clean_names() 

# Iterate loading csvs from directory and tidy data
longit_tidy = longit_file_names %>% 
  mutate(patient_data = map(path, read_csv)) %>% 
  unnest() %>% 
  gather(key = week, value = obs, week_1:week_8) %>% 
  separate(files, into = c("group_arm", "subject_id"), sep = "_") %>% 
  mutate(subject_id = str_replace(subject_id, ".csv", ""), 
         week = str_replace(week, "week_", "")) %>% 
  mutate(group_arm = factor(group_arm, levels = c("con", "exp")),
         subject_id = as.double(subject_id), 
         week = as.double(week)) %>% 
  select(group_arm, subject_id, week, obs)

```

### Spaghetti Plot of All Subjects Over Time

A spaghetti plot of all subjects from control and experimental groups over time reveals that control group observations are consistently lower than experimental group observations over time. The differences appear to be diverging more over time, with earlier time points showing more overlap between the groups and later time points showing greater differences. The observation values for the experimental group appears to be increasing over time.

```{r}
longit_tidy %>% 
  group_by(subject_id, group_arm) %>% 
  ggplot(aes(x = week, y = obs, color = group_arm, group = interaction(subject_id, group_arm))) + 
  geom_line(se = FALSE) +
  labs(
    title = "Observations for Each Subject in Control and Experimental Groups, Weeks 1-8",
    x = "Week",
    y = "Observation",
    caption = "Data from the Longitudinal Study"
  ) + 
  theme_bw()

```

## Problem 2

### Load and Clean the Data

The next problem explores data gathered by the Washington Post on homicides in 50 large US cities. Data was read in from a CSV posted on GitHub.

The data was collected from police departments selected for the survey. 

```{r}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_data = read_csv(url, na = "Unknown") %>% 
  janitor::clean_names() 

# Clean error in state "AL" to "OK" for Tul-000769
clean_homicide_data = homicide_data %>% 
  mutate(state = ifelse(uid == "Tul-000769", "OK", state)) %>% 
  mutate(city_state = str_c(city, ", ", state), 
         victim_race = fct_relevel(victim_race, "White"))

```

The dataset contains `r nrow(homicide_data)` observations and `r ncol(homicide_data)` variables. A new variable `city_state` was created to concatenate the city and state variables. The variables in the dataset are: 

*  `uid` - unique identifier
*  `reported_date` - date homicide reported
*  `victim_last` and `victim_first` - name of victim
*  `victim_race` - race of victim
*  `victim_age` - age of victim
*  `victim_sex` - sex of victim
*  `city` - city of homicide
*  `state` - state of homicide
*  `city_state` - combination city, state
*  `lat` and `lon` - latitude and longitude of homicide
*  `disposition` - outcome of homicide with three options: "Closed by arest", "Closed without arrest", or "Open/No arrest"

An error in the original data table listed "Tulsa, AL" as a location for one observation (Tul-000769); however, the coordinates actually corresponded to "Tulsa, OK". Therefore, the state was fixed to Oklahoma in the clean dataset.  

### Homicides by City, State

#### Total Homicides and Unsolved Homicides in All Cities
The total number of homicides and unsolved homicides were summarized for each city, state in a dataframe `homicide_counts`. The `n_total_homicides` provides a count for total homicides, and `n_unsolved_homicide` a count for unsolved homicides (those with disposition of “Closed without arrest” or “Open/No arrest”). 

```{r}
total_hom_count = clean_homicide_data %>% 
  group_by(city_state) %>% 
  summarize(n_total_homicides = n())

unsolved_hom_count = clean_homicide_data %>% 
  filter(disposition == "Closed without arrest" | disposition == "Open/No arrest") %>% 
  group_by(city_state) %>% 
  summarize(n_unsolved_homicide = n())

homicide_counts = full_join(total_hom_count, unsolved_hom_count, by = "city_state") %>% 
  arrange(desc(n_total_homicides))

```

Chicago, IL (`r filter(homicide_counts, city_state == "Chicago, IL") %>% pull(n_total_homicides)` homicides), Philadelphia, PA (`r filter(homicide_counts, city_state == "Philadelphia, PA") %>% pull(n_total_homicides)` homicides), and Houston, TX (`r filter(homicide_counts, city_state == "Houston, TX") %>% pull(n_total_homicides)` homicides) are the top three cities by total homicides. The table also presents the total number of unsolved homicides (those with disposition of “Closed without arrest” or “Open/No arrest”). Chicago, IL had the largest number of unsolved homicides with `r filter(homicide_counts, city_state == "Chicago, IL") %>% pull(n_unsolved_homicide)`. Baltimore, MD has the second largest number of unsolved homicides with `r filter(homicide_counts, city_state == "Baltimore, MD") %>% pull(n_unsolved_homicide)`. The city with the least unsolved homicides was Tampa, FL with `r filter(homicide_counts, city_state == "Tampa, FL") %>% pull(n_unsolved_homicide)`.

#### Unsolved Homicides in Baltimore, MD

A one sample test of the proportion of unsolved homicides in Baltimore, MD using `prop.test` was conducted. The default `prop.test` assumes a probability of success of 0.5, and confidence level of 95% (alpha = 0.05). The estimate and 95% confidence interval is presented in the following table. 

```{r}
baltimore_prop = prop.test(x = filter(homicide_counts, city_state == "Baltimore, MD") %>% pull(n_unsolved_homicide), 
          n = filter(homicide_counts, city_state == "Baltimore, MD") %>% pull(n_total_homicides)) %>% 
  broom::tidy()
    
baltimore_prop %>% select(estimate, conf.low, conf.high) %>% knitr::kable(digits = 5)

```

The high estimate proportion of `r baltimore_prop %>% pull(estimate)` unsolved homicides in Baltimore indicates that a large portion of homicides in Baltimore are often unresolved. 

#### Unsolved Homicides in All Cities

Next, the `prop.test` function was carried out to estimate the proportion of unsolved homicides for all cities. The `prop.test` function was used with a `map` function and list column approach. `broom::tidy` was used to clean the results from the test output.  

```{r}
homicide_prop_tests = homicide_counts %>% 
  mutate(prop_test = map2(.x = n_unsolved_homicide, .y = n_total_homicides, ~prop.test(x = .x, n = .y))) %>%
  mutate(prop_test = map(prop_test, broom::tidy)) %>% 
  unnest() %>% 
  select(city_state, estimate, conf.low, conf.high)
```

Next, a plot was created to visualize the estimated proportion of unsolved homicides by city/state. The graph was ordered by descending estimated proportions of unsolved homicides. The graph demonstrates that Chicago, New Orleans and Baltimore have the highest estimated proportion of unsolved homicides, while Richmond has the least. Tampa, FL falls fairly in the center of the graph. The error bar widths vary by city (due to different sample sizes), with cities such as San Bernadino having a wide interval, while Chicago has a narrow interval. 

These data are important for indicating which cities have the greatest estimated proportions of unsolved homicides. This information can be used to target resources towards reducing the number of unsolved homicides in the areas of greatest impact.  

```{r}
homicide_prop_tests %>%
  mutate(city_state = forcats::fct_reorder(city_state, desc(estimate))) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(x = city_state, ymin = conf.low, ymax = conf.high)) +
   theme(axis.text.x=element_text(angle = 90, hjust = 1, vjust = 0.5),
        text=element_text(size = 10)) +
  labs(title = "Estimated Proportion of Unsolved Homicides by City",
         x = "City, State",
         y = "Estimate (95% CI error bar)")

```

